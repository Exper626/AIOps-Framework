{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1ljpmJGYGSR"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "\n",
        "    for _ , param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "        print(f\"trainable_params : \", {trainable_params})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets huggingface_hub peft accelerate transformers bitsandbytes"
      ],
      "metadata": {
        "id": "PQuIlP5-YHZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "#del model  # or any other large objects\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "AI5U0xp_YI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import PeftModel, PeftConfig,  prepare_model_for_kbit_training,  LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"SECRET KEY\")\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = True,\n",
        "        bnb_4bit_use_double_quant = True,\n",
        "        bnb_4bit_quant_type = \"nf4\",\n",
        "        bnb_4bit_compute_dtype = torch.bfloat16\n",
        "        )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "\n",
        "config = LoraConfig(\n",
        "        r = 8,\n",
        "        lora_alpha = 32,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout = 0.05,\n",
        "        bias = \"none\",\n",
        "        task_type = \"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "with open(\"ocr.txt\", \"r\") as file:\n",
        "    lines = file.read().split(\"\\n\")\n",
        "\n",
        "lines = [line for line in lines if line.strip() != \"\"]\n",
        "raw_dataset = Dataset.from_dict({\"text\": lines})\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "trainer = Trainer (\n",
        "        model = model ,\n",
        "        train_dataset = tokenized_dataset,\n",
        "        args = TrainingArguments (\n",
        "            per_device_train_batch_size = 7,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 2,\n",
        "            num_train_epochs=20,  #\n",
        "            #max_steps = 10,\n",
        "            learning_rate = 2e-4,\n",
        "            fp16 = True,\n",
        "            logging_steps = 100,\n",
        "            output_dir = \"output_dir\",\n",
        "            optim = \"paged_adamw_8bit\",\n",
        "            ),\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()\n",
        "\n",
        "model_to_save = trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model\n",
        "model_to_save.save_pretrained(\"Mobitel\")"
      ],
      "metadata": {
        "id": "3EY9xZjeYM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda:0\")\n",
        "model.eval()\n",
        "\n",
        "while True:\n",
        "    question = input(\"Enter a question here: \")\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "NoFuhnmvYO3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}