{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhs3wYdwYc_T"
      },
      "outputs": [],
      "source": [
        "output_dir = \".\"\n",
        "pretrained_model_name_or_path   = \"runwayml/stable-diffusion-v1-5\"\n",
        "lora_rank = 4\n",
        "lora_alpha = 4\n",
        "learning_rate = 1e-4\n",
        "adam_beta1, adam_beta2 = 0.9, 0.999\n",
        "adam_weight_decay = 1e-2\n",
        "adam_epsilon = 1e-08\n",
        "dataset_name = None\n",
        "train_data_dir = \"./train_data\"\n",
        "top_rows = 4\n",
        "output_dir = \"output_dir\"\n",
        "resolution = 768\n",
        "center_crop = True\n",
        "random_flip = True\n",
        "train_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 200\n",
        "# The scheduler type to use. Choose between [\"linear\", \"cosine\", # \"cosine_with_restarts\", \"polynomial\",\"constant\", \"constant_with_\n",
        "# warmup\"]\n",
        "lr_scheduler_name = \"constant\" #\"cosine\"#\n",
        "max_grad_norm = 1.0\n",
        "diffusion_scheduler = DDPMScheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler = DDPMScheduler.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "weight_dtype = torch.float16\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    torch_dtype = weight_dtype\n",
        ").to(device)\n",
        "tokenizer, text_encoder = pipe.tokenizer, pipe.text_encoder\n",
        "vae, unet = pipe.vae, pipe.unet"
      ],
      "metadata": {
        "id": "F19I3gouYq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet.requires_grad_(False)\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)"
      ],
      "metadata": {
        "id": "RqoY3eMlYwaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_lora_config = LoraConfig(\n",
        "    r = lora_rank,\n",
        "    lora_alpha = lora_alpha,\n",
        "    init_lora_weights = \"gaussian\",\n",
        "    target_modules = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
        ")"
      ],
      "metadata": {
        "id": "G4BR0Rh3YxGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet.add_adapter(unet_lora_config)\n",
        "for param in unet.parameters():\n",
        "    # only upcast trainable parameters (LoRA) into fp32\n",
        "    if param.requires_grad:\n",
        "        param.data = param.to(torch.float32)"
      ],
      "metadata": {
        "id": "6nfmOcnPYzhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_captions(examples, is_train=True):\n",
        "    '''Preprocessing the datasets.We need to tokenize input captions and transform the images.'''\n",
        "    captions = []\n",
        "    for caption in examples[caption_column]:\n",
        "        if isinstance(caption, str):\n",
        "            captions.append(caption)\n",
        "    inputs = tokenizer(\n",
        "        captions,\n",
        "        max_length = tokenizer.model_max_length,\n",
        "        padding = \"max_length\",\n",
        "        truncation = True,\n",
        "        return_tensors = \"pt\"\n",
        "    )\n",
        "    return inputs.input_ids"
      ],
      "metadata": {
        "id": "MlBiLBY8Y2Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(\n",
        "            resolution,\n",
        "            interpolation=transforms.InterpolationMode.BILINEAR\n",
        "        ),\n",
        "        transforms.CenterCrop(resolution) if center_crop else\n",
        "            transforms.RandomCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip() if random_flip else\n",
        "            transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]) # [0,1] -> [-1,1]\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Y6d6gsTJY2iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_train(examples):\n",
        "    '''prepare the train data'''\n",
        "    images = [image.convert(\"RGB\") for image in examples[\n",
        "        image_column]]\n",
        "    examples[\"pixel_values\"] = [train_transforms(image)\n",
        "        for image in images]\n",
        "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "    return examples\n",
        "# only do this in the main process\n",
        "with accelerator.main_process_first():\n",
        "    # Set the training transforms\n",
        "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"]\n",
        "        for example in examples])\n",
        "    pixel_values = pixel_values.to(memory_format = \\\n",
        "        torch.contiguous_format).float()\n",
        "    input_ids = torch.stack([example[\"input_ids\"]\n",
        "        for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "# DataLoaders creation:\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle = True\n",
        "    collate_fn = collate_fn\n",
        "    batch_size = train_batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "nBgY5ztnY5lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
        "optimizer = torch.optim.AdamW(\n",
        "    lora_layers,\n",
        "    lr = learning_rate,\n",
        "    betas = (adam_beta1, adam_beta2),\n",
        "    weight_decay = adam_weight_decay,\n",
        "    eps = adam_epsilon\n",
        ")\n",
        "\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    lr_scheduler_name,\n",
        "    optimizer = optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "Glbcp9moY6AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_train_steps = num_train_epochs*len(train_dataloader)\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial = 0,\n",
        "    desc = \"Steps\",\n",
        "    # Only show the progress bar once on each machine.\n",
        "    Disable = not accelerator.is_local_main_process,\n",
        ")"
      ],
      "metadata": {
        "id": "uS9XUWYiY74d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_train_epochs):\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # step 1. Convert images to latent space\n",
        "        latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "        # step 2. Sample noise that we'll add to the latents,\n",
        "        latents provide the shape info.\n",
        "        noise = torch.randn_like(latents)\n",
        "        # step 3. Sample a random timestep for each image\n",
        "        batch_size = latents.shape[0]\n",
        "        timesteps = torch.randint(\n",
        "            low = 0,\n",
        "            high = noise_scheduler.config.num_train_timesteps,\n",
        "            size = (batch_size,),\n",
        "            device = latents.device\n",
        "        )\n",
        "        timesteps = timesteps.long()\n",
        "        # step 4. Get the text embedding for conditioning\n",
        "        encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "        # step 5. Add noise to the latents according to the noise\n",
        "        # magnitude at each timestep\n",
        "        # (this is the forward diffusion process),\n",
        "        # provide to unet to get the prediction result\n",
        "        noisy_latents = noise_scheduler.add_noise(\n",
        "            latents, noise, timesteps)\n",
        "        # step 6. Get the target for loss depend on the prediction\n",
        "        # type\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(\n",
        "                latents, noise, timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown prediction type {\n",
        "                noise_scheduler.config.prediction_type}\")\n",
        "        # step 7. Predict the noise residual and compute loss\n",
        "        model_pred = unet(noisy_latents, timesteps,\n",
        "            encoder_hidden_states).sample\n",
        "        # step 8. Calculate loss\n",
        "        loss = F.mse_loss(model_pred.float(), target.float(),\n",
        "            reduction=\"mean\")\n",
        "        # step 9. Gather the losses across all processes for logging\n",
        "        # (if we use distributed training).\n",
        "        avg_loss = accelerator.gather(loss.repeat(\n",
        "            train_batch_size)).mean()\n",
        "        train_loss += avg_loss.item() / gradient_accumulation_steps\n",
        "        # step 10. Backpropagate\n",
        "        accelerator.backward(loss)\n",
        "        if accelerator.sync_gradients:\n",
        "            params_to_clip = lora_layers\n",
        "            accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        # step 11. check optimization step and update progress bar\n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            train_loss = 0.0\n",
        "        logs = {\"epoch\": epoch,\"step_loss\": loss.detach().item(),\n",
        "            \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "        progress_bar.set_postfix(**logs)"
      ],
      "metadata": {
        "id": "Q9BREO01Y_MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_main_process:\n",
        "    unet = unet.to(torch.float32)\n",
        "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
        "    unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "        get_peft_model_state_dict(unwrapped_unet))\n",
        "    weight_name = f\"\"\"lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors\"\"\"\n",
        "    StableDiffusionPipeline.save_lora_weights(\n",
        "        save_directory = output_dir,\n",
        "        unet_lora_layers = unet_lora_state_dict,\n",
        "        safe_serialization = True,\n",
        "        weight_name = weight_name\n",
        "    )\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "bKrTaSQ6ZNCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}